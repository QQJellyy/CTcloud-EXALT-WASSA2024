{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code refers to EXALT baseline: https://github.com/pranaydeeps/WASSA24_EXALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import time\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "import torch\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "class CustomSequenceClassificationExplainer(SequenceClassificationExplainer): # need custom explainer to handle xlm-roberta\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        attribution_type: str = \"lig\",\n",
    "        custom_labels: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__(model, tokenizer)\n",
    "        \n",
    "    def _make_input_reference_token_type_pair(self, input_ids: torch.Tensor, sep_idx: int = 0\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns two tensors indicating the corresponding token types for the `input_ids`\n",
    "        and a corresponding all zero reference token type tensor.\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Tensor of text converted to `input_ids`\n",
    "            sep_idx (int, optional):  Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]\n",
    "        \"\"\"\n",
    "        seq_len = input_ids.size(1)\n",
    "        \n",
    "        if self.model.config.model_type == 'xlm-roberta':\n",
    "            token_type_ids = torch.zeros(seq_len, dtype=torch.int, device=self.device).expand_as(input_ids)\n",
    "        else:\n",
    "            token_type_ids = torch.tensor([0 if i <= sep_idx else 1 for i in range(seq_len)], device=self.device).expand_as(\n",
    "                input_ids\n",
    "            )\n",
    "        ref_token_type_ids = torch.zeros_like(token_type_ids, device=self.device).expand_as(input_ids)\n",
    "\n",
    "        return (token_type_ids, ref_token_type_ids)\n",
    "\n",
    "\n",
    "def Clean_AttributionTokens(tokenized_text, attributions):\n",
    "    \"\"\"creates a vector of binary values to indicate whether a word is a trigger word or not (based on a predefined threshold)\n",
    "\n",
    "    Args:\n",
    "        normalized_scores (list): a list of numerical scores that have already been normalized (i.e., they sum to 1.0)\n",
    "        threshold (float, optional): A lower bound for converting numerical values to a binary 1. Values below the threshold are converted to 0. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        list: a binary vector of 1s and 0s indicating whether a word is a trigger word or not\n",
    "    \"\"\"\n",
    "    offset_mapping = tokenizer(tokenized_text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "    #print(offset_mapping)\n",
    "    final_attributions = {}\n",
    "\n",
    "    # to ensure the same mapping, we need to find the indices of the spaces (which are token+1)\n",
    "    space_indices = [] # counting first character as a space because otherwise the first token will be skipped\n",
    "\n",
    "    # keep track of the space indices\n",
    "    for char_index, character in enumerate(tokenized_text + \" \"):  # add a space to capture the final token\n",
    "        #print(char_index, character)\n",
    "        if character.isspace():\n",
    "            #print(\"Space found\", char_index)\n",
    "            space_indices.append(char_index)\n",
    "\n",
    "    # not very effective to run over ALL mappings for EACH token, but it works\n",
    "    # for each space (i.e. token), find the corresponding sub-tokens and sum the attributions based on character\n",
    "    for i, space_index in enumerate(space_indices, start =0):\n",
    "        final_attributions[i] = 0\n",
    "        for tokenindex, mapping in enumerate(offset_mapping):\n",
    "            begin_index = mapping[0]\n",
    "            end_index = mapping[1]\n",
    "            if begin_index == 0 and end_index == 0: # ignore BoS and EoS tokens\n",
    "                continue\n",
    "            elif i == 0: # special treatment because there is no previous token for the first token\n",
    "                if space_index >= end_index: # any sub-tokens before space index (token delimiter) are concatenated \n",
    "                    final_attributions[i] += attributions[tokenindex]\n",
    "            \n",
    "            else:\n",
    "                if space_index >= end_index and begin_index >= space_indices[i-1]: # begin index > previous space index because otherwise importances will overlap\n",
    "                    final_attributions[i] += attributions[tokenindex]\n",
    "                elif space_index < begin_index:\n",
    "                    break\n",
    "    \n",
    "    #print(final_attributions)\n",
    "    final_outputs = []\n",
    "    for key in final_attributions.keys():\n",
    "        final_outputs.append(final_attributions[key])\n",
    "    return tokenized_text.split(\" \"), final_outputs\n",
    "\n",
    "def Vector_from_raw_attributions(inputstring, interpret_output, threshold=0.1):\n",
    "    \"\"\" a combined function to normalize, clean, and create a binary vector from raw numerical attributions values for subtokens\n",
    "    Args:\n",
    "        inputstring (string): the input text string (from the column [\"Texts\"])\n",
    "        interpret_output (Tuple): a Tuple containing the output from the importance attribution model (i.e., the raw attributions + the subtokens)\n",
    "        threshold (float, optional): Minimal required importance to convert the numerical value to a binary 1. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        list: a vector of binary values indicating whether a token is a trigger word or not\n",
    "    \"\"\"\n",
    "    sub_tokens, attribute_scores = zip(*interpret_output)\n",
    "    tokenized_sample, attribute_scores = Clean_AttributionTokens(inputstring, attribute_scores)\n",
    "    return tokenized_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /work/share/zjh/Huize/test/infer-hw/__pycache__/emo/resultT/xlm-roberta-large=exalt_emotion_train=fullPT=lr4e5=bs256=rand=bf16=0510-231202/checkpoint-387 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# to run this on the entire dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# pick the model you want to use\n",
    "MODEL_NAME = ''\n",
    "\n",
    "# load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to('cuda:0')\n",
    "cls_explainer = CustomSequenceClassificationExplainer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@user', 'I’m', 'so', 'happy', 'you’ve', 'found', 'some', 'success', 'for', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "traindata = pd.read_csv(\"data/raw/exalt_triggers_train.tsv\", sep=\"\\t\")\n",
    "\n",
    "tokens = []\n",
    "for rowindex, row in traindata[:].iterrows():\n",
    "    tweet_text = row[\"Texts\"]\n",
    "    interpret_output = cls_explainer(tweet_text)\n",
    "    final_tokens = Vector_from_raw_attributions(tweet_text, interpret_output,thr)\n",
    "    tokens.append(final_tokens)\n",
    "    break\n",
    "\n",
    "print(final_tokens)\n",
    "# traindata[\"tokens\"] = tokens\n",
    "# traindata.to_csv(\"data/raw/exalt_triggers_test_participants_token.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
